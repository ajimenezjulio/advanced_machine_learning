{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# port AML_CW2 to notebook file format for web browsing and pdf conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# machine learning libraries\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import classifiers as clf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# file handling libraries\n",
    "import glob\n",
    "import os\n",
    "from os.path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data.mat file...\n"
     ]
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "\n",
    "data_file_name = 'spham_data.mat'\n",
    "\n",
    "if not exists(join(current_dir + data_file_name)):\n",
    "\n",
    "    print(\"Creating data.mat file...\")\n",
    "              \n",
    "    file_paths = [current_dir + '/data/' + folder + '*.txt' for folder in [\"spam/\", \"ham/\", \"test/\"]]\n",
    "\n",
    "    spam = glob.glob(file_paths[0])\n",
    "    ham  = glob.glob(file_paths[1])\n",
    "    test = glob.glob(file_paths[2])\n",
    "              \n",
    "    # Construct vectorizer for TF-IDF feature extraction \n",
    "    vectorizer = TfidfVectorizer(input='filename',\n",
    "                                 lowercase = True, \n",
    "                                 stop_words=\"english\",\n",
    "                                 encoding='latin-1', \n",
    "                                 min_df = 8)\n",
    "    \n",
    "    spham = spam + ham\n",
    "    tr = vectorizer.fit_transform(spham) # fit_transform returns a term-document matrix object\n",
    "    ts = vectorizer.transform(test) \n",
    "              \n",
    "    dicy = {\n",
    "        'training_labels': [1] * len(spam) + [0] * len(ham),\n",
    "        'training_data': tr, \n",
    "        'test_data': ts\n",
    "    }\n",
    "\n",
    "    # Saving dictionary as variable to reuse if necessary\n",
    "    scipy.io.savemat('data.mat', dicy)\n",
    "\n",
    "# Loading data\n",
    "data = scipy.io.loadmat(data_file_name)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5857,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_prediction = clf.svm(data, 0.1)\n",
    "print(svm_prediction.shape)\n",
    "svm_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.setX(data['training_data'])\n",
    "\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "# ntrees = The number of trees in the forest\n",
    "ntrees = 100\n",
    "res_rforest = clf.rndForest(data, ntrees)\n",
    "\n",
    "# Logistic Regression\n",
    "# C = Inverse of regularization strength\n",
    "C = 1.0\n",
    "res_lg = clf.logReg(data, C)\n",
    "\n",
    "# Extreme Gradient Boost\n",
    "# md = Max depth\n",
    "# ss = Subsample ratio of training instance\n",
    "# cs = Subsample ratio of columns when constructing each tree\n",
    "md = 1\n",
    "ss = 0.8\n",
    "cs = 0.8\n",
    "res_xgb = clf.xgboost(data, md, ss, cs)\n",
    "\n",
    "# Linear Discriminant Analysis\n",
    "res_lda = clf.lda(data)\n",
    "\n",
    "# Quadratic Discriminant Analysis\n",
    "res_qda = clf.qda(data)\n",
    "\n",
    "# Ada Boost\n",
    "# ne = Maximum number of estimators at which boosting is terminated\n",
    "ne = 50\n",
    "res_ada = clf.adaboost(data, ne)\n",
    "\n",
    "# Building array of results\n",
    "results = [res_svm, res_rforest, res_lg, res_xgb, res_lda, res_qda, res_ada]\n",
    "\n",
    "print(\"Writing Results...\")\n",
    "# Saving results\n",
    "createCSV(res_svm, 'resSVM')\n",
    "createCSV(res_rforest, 'resRndForest')\n",
    "createCSV(res_lg, 'resLogReg')\n",
    "createCSV(res_xgb, 'resXGBoost')\n",
    "createCSV(res_lda, 'resLDA')\n",
    "createCSV(res_qda, 'resQDA')\n",
    "createCSV(res_ada, 'resADA')\n",
    "\n",
    "# 0 as second parameter to evaluat 'Ham' accuracy\n",
    "printRes(results, 0) \n",
    "print(len(res_svm))\n",
    "print(\"\\n ****** Finished ******\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
